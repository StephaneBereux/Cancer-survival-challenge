{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Breast cancer survival prediction</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "    1. [Survival analysis](#survival_analysis)\n",
    "        1. [A few definitions](#definitions)\n",
    "        2. [The Kaplan-Meier estimator](#kaplan_meier)\n",
    "        3. [The log-rank test](#log_rank_test)\n",
    "        4. [The Cox regression](#cox_regression)\n",
    "2. [Data exploration](#data_exploration)\n",
    "    1. [Python requirements](#python_requirements)\n",
    "    2. [Getting the data](#getting_data)\n",
    "    3. [Baseline model](#baseline_model)\n",
    "3. [Submission](#submission)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "Breast cancer is one of the most common cancers and the second leading cause of cancer death among women in the United States. One in nine women will be diagnosed with breast cancer in her lifetime ([INCa avril 2016](https://www.ligue-cancer.net/article/26094_cancer-du-sein)). Approximately 70% of breast cancer patients are inoperable because of advanced tumor growth or bone metastasis [Min Tao et al.](https://pubmed.ncbi.nlm.nih.gov/21512769/). \n",
    "\n",
    "It is therefore crucial to be able to accurately diagnose the disease, and to better understand the aggravating factors. Here we propose to predict the survival based on the genetic factors of the tumour.\n",
    "\n",
    "### Survival analysis <a name=\"survival_analysis\"></a>\n",
    "\n",
    "*[Survival analysis](https://en.wikipedia.org/wiki/Survival_analysis)* is a branch of statistics designed to model the life or activity time of a living being or device. Widely used in medicine and particularly in oncology, it allows to process data often *censored* over time. As far as oncology is concerned, for instance, it is inevitable that patients become out of reach or leave the study before the measured event (death, remission, metastasis, etc.): classic statistical tools such as regression are then poorly adapted and may underestimate the lifetimes studied. Here we intend to provide a brief and non-exhaustive overview of basic relevant statistical techniques that might be useful for the rest of the challenge.\n",
    "\n",
    "#### A few definitions <a name=\"definitions\"></a>\n",
    "\n",
    "To make our introduction more readable, we can assume that time is a discrete variable i.e $t \\in \\{1,...,n\\}$ with $n \\in \\mathbb{N}$.\n",
    "\n",
    "\n",
    "Let $ \\tau \\geq 0$ be the random variable giving the time before a relevant event happen. We then wish to estimate the *survival function* $S$ which define  $\\tau$, where $S$ is defined by :\n",
    "$ {\\displaystyle S(t)=\\mathrm {Prob} (\\tau >t)}$\n",
    "\n",
    "\n",
    "We also make the assumption that the censors (the patients leaving the study) are non-informative (e.g. they don't have anything to do with the subject studied, e.g. can be considered random).\n",
    "\n",
    "#### The Kaplan-Meier estimator <a name=\"kaplan_meier\"></a>\n",
    "\n",
    "The Kaplan-Meier estimator is one of the most classic tool of survival analysis. Indeed, it is a non-parametric estimator (eg.g it does not make any assumption on the data distribution accross time), quite simple to implement, and it uses the information from the censored lineage.\n",
    "The Kaplan-Meier estimator can be written : \n",
    "${\\displaystyle {\\hat{S}}(t)=\\prod_{i:t_{i}\\leq t}\\left(1-{\\frac{d_{i}}{n_{i}}}\\right)}$. \n",
    " \n",
    "where $t_i$ is a time when at least one event happened, $d_i$ the number of event at time ${\\displaystyle t_{i}}$ and $n_i$ the individuals still in the study (neither dead nor censored) at time $t_i$.\n",
    "\n",
    "Intuitively, the product to compute ${\\hat{S}}(t)$ is on all time less than t, and thus allows to take into account all patients, even those *censored* before time $t$ because they got out of the reach of the study.\n",
    "\n",
    "\n",
    "Visually, a representation of a Kaplan-Meier estimator gives a decreasing staircase function, depicting an estimation of the survival function $S(t)$ ($S$ is supposed to be constant between two events).\n",
    "\n",
    "By allowing to estimate the survival function of a sample, the Kaplan-Meier estimator enables the comparison between two different samples (for instance between a medicated group and a control group). But to this aim, other methods can be more appropriate.\n",
    "\n",
    "#### The log-rank test <a name=\"log_rank_test\"></a>\n",
    "\n",
    "The log-rank test is a non-parametric hypothesis test allowing to compare the survival functions of two different populations. In practice, it's used to quantify the survival differences between two populations which can be observed by plotting their Kaplan-Meier estimation.\n",
    "\n",
    "Indeed, the log-rank test also allows to take into account censored lineage without loosing to much information.\n",
    "\n",
    "A few notations :\n",
    "\n",
    "For each time j, we write $N_{{1j}}$ and $N_{{2j}}$ the number of patients from each group in each group still studied at time j, and $N_{j}=N_{{1j}}+N_{{2j}}$. Let's write $O_{{1j}}$ and $O_{{2j}}$ the number of events (e.g deaths) in each group happening at time j, and similarly $O_{j}=O_{{1j}}+O_{{2j}}$.\n",
    "\n",
    "The idea of the log-rank test is to test the null hypothesis \"the two population have the same survival function\".\n",
    "\n",
    "Under this hypothesis, $O_{1j}$ follows an hypergeometric law with parameters $N_j$, $N_{1j}$ and $O_j$, and thus with expectation $E_{1j}$ and variance $V_j$.\n",
    "\n",
    "The convergence being garanteed by the Central Limit Theorem (Lyapunov version), we can define the random variable $${\\displaystyle Z={\\frac {\\sum _{j=1}^{J}(O_{1j}-E_{1j})}{\\sqrt {\\sum _{j=1}^{J}V_{j}}}}{\\xrightarrow {\\mathbb{L}}\\ N(0,1).}}$$\n",
    "\n",
    "It is then easy to quantify the ressemblance between the law of $Z$ and a standard normal law to see if the null hypothesis can be rejected.\n",
    "\n",
    "#### The Cox regression<a name=\"cox_regression\"></a>\n",
    "\n",
    "The Cox regression is the equivalent of the logistic regression for survival analysis.\n",
    "Let's define the *hazard rate*\n",
    "$$\\lambda(t)=\\lim _{d t \\rightarrow 0} \\frac{\\operatorname{Pr}(t \\leq T<t+d t)}{d t \\cdot S(t)}=-\\frac{S^{\\prime}(t)}{S(t)}$$\n",
    "defined as the probability of dying at a certain time t given the fact that the patient survived until time t.\n",
    "The Cox regression belong to the class of *proportional hazard models*, that is models which make the assumption that each unit increase for a covariate multiply the hazard rate of the patient by a fixed amount. For instance, taking a certain drug could halve the probability of dying from cancer at each time t.\n",
    "\n",
    "This idea is formalized this way : \n",
    "\n",
    "Let $X_{i}=\\left\\{X_{i 1}, \\ldots X_{i p}\\right\\}$ be the covariate values for a certain patient i.\n",
    "Then we model the hazard rate $\\lambda$ by \n",
    "<p style=\"color:#FF0000\";>$${\\lambda\\left(t | X_{i}\\right)=\\lambda_{0}(t) \\exp \\left(\\beta_{1} X_{i 1}+\\cdots+\\beta_{p} X_{i p}\\right)=\\lambda_{0}(t) \\exp \\left(X_{i} \\cdot \\beta\\right)}$$</p>.\n",
    "\n",
    "Let's  note that only the base hazard rate is time-dependant : the effect of each covariate on the *hazard rate* is assumed to be the same across time.\n",
    "\n",
    "This property is the main \"trick\" of Cox regression : it allows to get rid of the hard-to-estimate hazard rate by taking a ratio. To be more precise, the likelihood of an event to be observed on patient i at time $Y_i$ is $$L_{i}(\\beta)=\\frac{\\lambda\\left(Y_{i} | X_{i}\\right)}{\\sum_{j : Y_{j} \\geq Y_{i}} \\lambda\\left(Y_{i} | X_{j}\\right)}=\\frac{\\lambda_{0}\\left(Y_{i}\\right) \\theta_{i}}{\\sum_{j : Y_{j} \\geq Y_{i}} \\lambda_{0}\\left(Y_{i}\\right) \\theta_{j}}=\\frac{\\theta_{i}}{\\sum_{j : Y_{j} \\geq Y_{i}} \\theta_{j}},$$ with $\\theta_{j}=\\exp \\left(X_{j} \\cdot\\beta\\right)$ and the summation being on all the patient still alive and in the study at time $Y_i$.\n",
    "\n",
    "Skipping the most technical details, the idea is then to model all patients as independant and to maximize the product likekihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "Parler du [survival analysis](https://en.wikipedia.org/wiki/Survival_analysis) , en particulier avec des données censurées à droite (ce qui est notre cas : cela correspond à toutes les données pour lesquelles on a ```event = 0 ```, c'est à dire qu'on a perdu la trace du patient). On peut se baser sur toute la doc de ces deux libraries Python, qui englobe tous les points de notre projet, avec notamment des exemples d'analyse de A à Z, depuis la data exploration jusqu'au choix du modèle et de la métrique : \n",
    "\n",
    "+ scikit-survival\n",
    "+ pysurvival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration <a name=\"data_exploration\"></a>\n",
    "\n",
    "### Python requirements <a name=\"python_requirements\"></a>\n",
    "\n",
    "In order to collect and analyse the data, the following Python libraries are required :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Generic requirements\n",
      "numpy\n",
      "pandas\n",
      "scipy\n",
      "sklearn\n",
      "\n",
      "# Gather the data\n",
      "mygene\n",
      "xenaPython\n",
      "\n",
      "# Survival analysis\n",
      "scikit-survival\n"
     ]
    }
   ],
   "source": [
    "with open('requirements.txt', 'r') as requirements:\n",
    "    print(requirements.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which can be installed (under Linux) with :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather the data <a name=\"getting_data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you have to download the data, by running :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querying 1-1000...done.\n",
      "querying 1001-2000...done.\n",
      "querying 2001-3000...done.\n",
      "querying 3001-4000...done.\n",
      "querying 4001-5000...done.\n",
      "querying 5001-6000...done.\n",
      "querying 6001-7000...done.\n",
      "querying 7001-8000...done.\n",
      "querying 8001-9000...done.\n",
      "querying 9001-10000...done.\n",
      "querying 10001-11000...done.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"download_data.py\", line 226, in <module>\n",
      "    check_previous_download()\n",
      "  File \"download_data.py\", line 183, in check_previous_download\n",
      "    download_data()\n",
      "  File \"download_data.py\", line 164, in download_data\n",
      "    df_expression = get_expression_data(hub, dataset, samples)\n",
      "  File \"download_data.py\", line 123, in get_expression_data\n",
      "    genes = available_genes(hub, dataset, fields)\n",
      "  File \"download_data.py\", line 57, in available_genes\n",
      "    dico_genes = mg.get_client(\"gene\").getgenes(genes, 'symbol')\n",
      "  File \"/home/stephane/anaconda3/envs/sb/lib/python3.8/site-packages/biothings_client/base.py\", line 387, in _getannotations\n",
      "    for hits in self._repeated_query(query_fn, ids, verbose=verbose):\n",
      "  File \"/home/stephane/anaconda3/envs/sb/lib/python3.8/site-packages/biothings_client/base.py\", line 230, in _repeated_query\n",
      "    time.sleep(self.delay)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "#!python download_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data are missing. Maybe you should first run \n",
      " \" python download_data.py \"\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/stephane/anaconda3/envs/sb/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from problem import get_train_data, get_test_data\n",
    "X_train, y_train = get_train_data()\n",
    "X_test, y_test = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b9c382e5e2fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mproblem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_test_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datascience/Datacamp/Cancer-survival-challenge/problem.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0mworkflow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_regressor_df_worflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m score_types = [\n\u001b[0;32m--> 184\u001b[0;31m     \u001b[0mConcordanceIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'concordance_index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m ]   \n",
      "\u001b[0;32m~/Datascience/Datacamp/Cancer-survival-challenge/problem.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, precision)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mminimum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mmaximum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0my_tot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_y_tot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_tot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Max survival time in the whole dataset (same as in the train dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datascience/Datacamp/Cancer-survival-challenge/problem.py\u001b[0m in \u001b[0;36m_get_y_tot\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_y_tot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;34m\"\"\"Return the concatenation of the train and the test labels.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_tot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datascience/Datacamp/Cancer-survival-challenge/problem.py\u001b[0m in \u001b[0;36mget_train_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;34m\"\"\"Return the train data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datascience/Datacamp/Cancer-survival-challenge/problem.py\u001b[0m in \u001b[0;36m_read_data\u001b[0;34m(path, dir_name)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mpath_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_database\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'X.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mpath_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_database\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mcheck_data_exist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_database\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datascience/Datacamp/Cancer-survival-challenge/problem.py\u001b[0m in \u001b[0;36mcheck_data_exist\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data are missing. Maybe you should first run \\n \" python download_data.py \"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSystemExit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : Although 55511 genes were listed, the data for \"only\" 37496 genes are available.\n",
    "\n",
    "We also note the two last columns :\n",
    "\n",
    "+ ```death``` can take two values : \n",
    "    + ```1``` means the patient died\n",
    "    + ```0``` means the patient was censored\n",
    "+ ```time``` is equal to the time of the event (either death or censoring)\n",
    "\n",
    "Thus, instead of having two variables ```X``` and ```y```, we have three variables ```X```, ```y``` and ```E``` constructed as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['death'].value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first feature : the correlation with the survival time\n",
    "\n",
    "A first idea would be to select only those genes that are most correlated, in absolute terms, with survival time. To do this, uncensored patients are filtered out and the correlation with survival time of each gene from the remaining patients is computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(columns=['index'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_uncensored = X_train[y_train['death']==1].drop(columns=['death'])\n",
    "y_uncensored = y_train[y_train['death']==1]\n",
    "print('They are %i uncensored patients' % len(X_uncensored))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the correlation for those patients.\n",
    "\n",
    "Pearson correlation relies on the assumption that the variable are a normally distributed, which is clearly not the case here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,6))\n",
    "sns.histplot(y_uncensored['time'].to_numpy(), ax=ax1, stat='probability',bins=25)\n",
    "ax1.set_title('Distribution of the time to death')\n",
    "ax1.set_xlabel('Time to death')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "log_time = np.log(y_uncensored['time'].to_numpy())\n",
    "sns.histplot(log_time, ax=ax2, stat='probability', color='r', bins=25)\n",
    "ax2.set_title('Distribution of the logarithm of the time to death')\n",
    "ax2.set_xlabel('Time to death (log)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logarithmic transformation slightly improves the time distribution and makes it a little more normal.\n",
    "Let's compute the correlation of the expression of each gene with the time to death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.stats as stats\n",
    "\n",
    "df_log_time = pd.Series(log_time)\n",
    "\n",
    "# First we remove the mean and we scale each gene expression data to unit variance\n",
    "scaler = StandardScaler()\n",
    "scaled_X_train = scaler.fit_transform(X_uncensored)\n",
    "scaled_df = pd.DataFrame(data=scaled_X_train)\n",
    "\n",
    "# Then we compute the correlations\n",
    "correlations = scaled_df.corrwith(df_log_time)\n",
    "np_correlations = correlations.dropna().to_numpy().reshape(-1,1)\n",
    "\n",
    "sns.histplot(np_correlations, stat='probability', color='green')\n",
    "plt.title('Distribution of the correlation between the genes expression and the time before death')\n",
    "plt.xlabel('Correlation value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, the distribution of correlations seems close to the normal law... The comparison between the two laws can be refined by means of a Q-Q plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_correlations = scaler.fit_transform(np_correlations)\n",
    "stats.probplot(scaled_correlations.flatten(), dist=\"norm\",plot=plt)\n",
    "plt.title('QQ-plot between the normal distribution and the correlation distribution.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "corr_r, p_values = [], []\n",
    "print(log_time.shape)\n",
    "for gene in scaled_X_train.T:\n",
    "    r, p_val = pearsonr(gene, log_time)\n",
    "    corr_r.append(r)\n",
    "    p_values.append(p_val)\n",
    "sns.histplot(p_values, stat='probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(corr_r, stat='probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(p_values, dist=\"uniform\",plot=plt)\n",
    "plt.title('QQ-plot between the uniform distribution and the p-values distribution.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = X_train.drop(columns=['death'])\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,6))\n",
    "sns.histplot(y_train['time'].to_numpy(), ax=ax1, stat='probability',bins=30)\n",
    "ax1.set_title('Distribution of the time to death')\n",
    "ax1.set_xlabel('Time to death')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "log_time = np.log(y_train['time'].to_numpy())\n",
    "sns.histplot(log_time, ax=ax2, stat='probability', color='r', bins=30)\n",
    "ax2.set_title('Distribution of the logarithm of the time to death')\n",
    "ax2.set_xlabel('Time to death (log)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_time = pd.Series(log_time)\n",
    "\n",
    "# First we remove the mean and we scale each gene expression data to unit variance\n",
    "scaler = StandardScaler()\n",
    "scaled_X_train = scaler.fit_transform(X_all)\n",
    "scaled_df = pd.DataFrame(data=scaled_X_train)\n",
    "\n",
    "# Then we compute the correlations\n",
    "correlations = scaled_df.corrwith(df_log_time)\n",
    "np_correlations = correlations.dropna().to_numpy().reshape(-1,1)\n",
    "\n",
    "sns.histplot(np_correlations, stat='probability', color='green')\n",
    "plt.title('Distribution of the correlation between the genes expression and the time before death')\n",
    "plt.xlabel('Correlation value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_correlations = scaler.fit_transform(np_correlations)\n",
    "stats.probplot(scaled_correlations.flatten(), dist=\"norm\",plot=plt)\n",
    "plt.title('QQ-plot between the normal distribution and the correlation distribution.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_r, p_values = [], []\n",
    "for gene in scaled_X_train.T:\n",
    "    r, p_val = pearsonr(gene, log_time)\n",
    "    corr_r.append(r)\n",
    "    p_values.append(p_val)\n",
    "sns.histplot(p_values, stat='probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(p_values, dist=\"uniform\",plot=plt)\n",
    "plt.title('QQ-plot between the uniform distribution and the p-values distribution.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ideas :\n",
    "+ Study the gene expression distributions, those who are very expressed, those who aren't...\n",
    "+ Study the correlations among the gene expression\n",
    "+ Study the difference of gene expression (those with the biggest difference for example) between alive patients and dead patients\n",
    " \n",
    " ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building training and testing sets\n",
    "N = len(df1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "index_train, index_test = train_test_split( range(N), test_size = 0.2, random_state=10)\n",
    "data_train = df1.loc[index_train].reset_index( drop = True )\n",
    "data_test  = df1.loc[index_test].reset_index( drop = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train['death'].value_counts())\n",
    "print(data_test['death'].value_counts())\n",
    "E_train, E_test = data_train.pop('death'), data_test.pop('death')\n",
    "y_train, y_test = data_train.pop('time'), data_test.pop('time')\n",
    "X_train, X_test = data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysurvival.models.survival_forest import ConditionalSurvivalForestModel\n",
    "from pysurvival.models.semi_parametric import CoxPHModel\n",
    "from pysurvival.utils.metrics import concordance_index\n",
    "from pysurvival.utils.metrics import integrated_brier_score as ib_score\n",
    "from pysurvival.utils.display import integrated_brier_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X_train)\n",
    "X_train_transformed = pca.transform(X_train)\n",
    "X_test_transformed = pca.transform(X_test)\n",
    "\n",
    "#lr = LinearRegression().fit(X_train, y_train)\n",
    "#c_index = concordance_index(lr, X_test_transformed, y_test, E_test)\n",
    "#print('C-index: {:.2f}'.format(c_index)) #0.1\n",
    "#ibs = ib_score(lr, X_test_transformed, y_test, E_test, t_max=max_time)\n",
    "#print(ibs)\n",
    "\n",
    "coxph = CoxPHModel()\n",
    "coxph.fit(X_train_transformed, y_train, E_train, lr=0.1, l2_reg=1e-1, max_iter=1000)\n",
    "c_index = concordance_index(coxph, X_test_transformed, y_test, E_test)\n",
    "print('C-index: {:.2f}'.format(c_index)) #0.1\n",
    "ibs = ib_score(coxph, X_test_transformed, y_test, E_test, t_max=max_time)\n",
    "print('IBS: {:.9f}'.format(ibs)) \n",
    "integrated_brier_score(coxph, X_test_transformed, y_test, E_test, t_max=max_time, figure_size=(20, 6.5))\n",
    "\n",
    "# Fitting the model\n",
    "csf = ConditionalSurvivalForestModel(num_trees=10)\n",
    "csf.fit(X_train_transformed, y_train, E_train, max_features='sqrt')\n",
    "\n",
    "from pysurvival.utils.display import display_loss_values\n",
    "#display_loss_values(csf, figure_size=(7, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysurvival.models.multi_task import LinearMultiTaskModel\n",
    "import numpy as np\n",
    "lmtm = LinearMultiTaskModel(bins=200)\n",
    "lmtm.fit(X_train_transformed, y_train, E_train, lr=0.0001, l2_reg=1e-1,num_epochs=1000)\n",
    "print(lmtm.times)\n",
    "c_index = concordance_index(lmtm, X_test_transformed, y_test, E_test)\n",
    "print('C-index: {:.2f}'.format(c_index)) #0.1\n",
    "ibs = ib_score(lmtm, X_test_transformed, y_test, E_test, t_max=max_time)\n",
    "print('IBS: {:.9f}'.format(ibs)) \n",
    "integrated_brier_score(lmtm, X_test_transformed, y_test, E_test, t_max=max_time, figure_size=(20, 6.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = lmtm.predict_survival(X_test_transformed)\n",
    "ibs = ib_score(i, X_test_transformed, y_test, E_test, t_max=max_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_melt = pd.concat([E_train, y_train], axis=1)\n",
    "print(df_to_melt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb as pdb\n",
    "def to_structured_array(E_df, y_df):\n",
    "    E = E_df.to_numpy().astype(bool)\n",
    "    y = y_df.to_numpy()\n",
    "    w = np.column_stack((E, y))\n",
    "    w = w.ravel().view([('event', w[0].dtype), ('time', y.dtype)]).astype('bool, <i8')\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sksurv.metrics import concordance_index_ipcw\n",
    "risk = lmtm.predict_risk(X_test_transformed)\n",
    "s_train, s_test = to_structured_array(E_train, y_train), to_structured_array(E_test, y_test)\n",
    "concordance_index_ipcw(s_train, s_test, risk)\n",
    "test_risk = - y_test + 4500\n",
    "print(concordance_index_ipcw(s_train, s_test, test_risk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysurvival.utils.metrics import concordance_index\n",
    "c_index = concordance_index(csf, X_test_transformed, y_test, E_test)\n",
    "print('C-index: {:.2f}'.format(c_index)) #0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysurvival.utils.display import integrated_brier_score\n",
    "ibs = integrated_brier_score(csf, X_test_transformed, y_test, E_test, t_max=10000,\n",
    "                       figure_size=(20, 6.5) )\n",
    "print('IBS: {:.9f}'.format(ibs)) #0.92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model <a name=\"baseline_model\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric\n",
    "\n",
    "We could use Integrated Brier Score (https://square.github.io/pysurvival/metrics/brier_score.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission <a name=\"submission\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
