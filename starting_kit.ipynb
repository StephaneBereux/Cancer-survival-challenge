{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Breast cancer survival prediction</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "    1. [Basic cellular biology concepts](#base_biology)\n",
    "        1. [Genetic information](#genetic_information)\n",
    "        2. [Transcriptome](#transcriptome)\n",
    "        3. [Gene network](#gene_network)\n",
    "    2. [Cancer](#cancer)\n",
    "    1. [Survival analysis](#survival_analysis)\n",
    "        1. [A few definitions](#definitions)\n",
    "        2. [The Kaplan-Meier estimator](#kaplan_meier)\n",
    "        3. [The log-rank test](#log_rank_test)\n",
    "        4. [The Cox regression](#cox_regression)\n",
    "2. [Data exploration](#data_exploration)\n",
    "    1. [Python requirements](#python_requirements)\n",
    "    2. [Getting the data](#getting_data)\n",
    "    3. [Baseline model](#baseline_model)\n",
    "3. [Submission](#submission)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "Breast cancer is one of the most common cancers and the second leading cause of cancer death among women in the United States. One in nine women will be diagnosed with breast cancer in her lifetime ([INCa avril 2016](https://www.ligue-cancer.net/article/26094_cancer-du-sein)). Approximately 70% of breast cancer patients are inoperable because of advanced tumor growth or bone metastasis [Min Tao et al.](https://pubmed.ncbi.nlm.nih.gov/21512769/). \n",
    "\n",
    "It is therefore crucial to be able to accurately diagnose the disease, and to better understand the aggravating factors. Here we propose to predict the survival based on the genetic factors of the tumour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic cellular biology concepts <a name=\"base_biology\"></a>\n",
    "\n",
    "The following explanation is deliberately simplified, and does not get bogged down in unnecessary biological details that are irrelevant to the challenge. \n",
    "It simply aims to present a schematic vision of the functioning of the majority of human cells*, which justifies the approach adopted in this challenge, and thus gives possible leads for the participants.\n",
    "\n",
    "\\**except in particular degenerated cells such as red blood cells...*\n",
    "\n",
    "#### Genetic information <a name=\"genetic_information\"></a>\n",
    "\n",
    "The human being is made up of approximately $10^{13}$ cells. These cells are not immutable: on the contrary they are themselves living entities, which are born, live and die.\n",
    "\n",
    "Each human cell has its genetic information contained in [DNA](https://en.wikipedia.org/wiki/DNA#Biological_functions), a molecule with excellent storage capacities thanks to its stability and its faithful transmission from the mother cell to the daughter cells. This information is divided into genes.\n",
    "\n",
    "During its life, the cell is led to express part of this information (e.g. part of the genes), by translating it into molecular tools. \n",
    "The modalities of expression differ according to the genes, but for the vast majority, the beginning of the expression process is the same. \n",
    "\n",
    "#### Transcriptome <a name=\"transcriptome\"></a>\n",
    "\n",
    "To be expressed, each gene is first [transcribed](https://en.wikipedia.org/wiki/Transcription_(biology)) into multiple copies of itself, in the form of [RNA](https://en.wikipedia.org/wiki/RNA#Comparison_with_DNA) molecules (a molecule close to DNA). Each copy of RNA contains the same genetic information as the original gene, but in a more reactive (and therefore more ephemeral) form than DNA. This makes it more accessible for expression.\n",
    "\n",
    "![central-dogma.png](img/central-dogma.png)\n",
    "\n",
    "<center><u>Pipeline of the expression of the genetic information : transcription into RNA and translation into proteins</u></center>\n",
    "\n",
    "*(from https://www.atdbio.com/)*\n",
    "\n",
    "*(The further expression of the genetic information is beyond the scope of this challenge, but essentially consists of two possibilities: either the RNA molecule is directly used as a molecular tool, or an additional step takes place ([translation](https://en.wikipedia.org/wiki/Translation_(biology))) which translates the RNA molecule into a protein.)*\n",
    "\n",
    "The expression of a gene is therefore controlled by the number of RNA copies produced for each gene. The set of RNA molecules in a cell is called the [transcriptome](https://en.wikipedia.org/wiki/Transcriptome). \n",
    "Here we (classically) propose to use the transcriptome as a proxy to infer the expression of each gene.\n",
    "\n",
    "As you will see in the following part, the main difficulty of this challenge will be to reduce the dimension of the very high-dimensional transcriptomic data (expression data for more than 35000 genes), in order to accurately predict the survival time of breast cancer patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genes network  <a name=\"gene_network\"></a>\n",
    "\n",
    "Genes can interact with each other, which mean that some genes will induce or inhibits the expression of other genes. [Inferring these relationships](https://en.wikipedia.org/wiki/Gene_regulatory_network) has been an active area of research, and the [results](https://www.researchgate.net/publication/8910145_Network_Biology_Understanding_The_Cell%27s_Functional_Organization) tend to show that this network is a [hierarchical scale-free network](https://en.wikipedia.org/wiki/Hierarchical_network_model), hence a rather sparse network containing only some highly connected nodes, surrounded by local dense clusters.\n",
    "\n",
    "![hierarchical](img/hierarchical.jpg)\n",
    "<center><u>Example of hierarchical scale-free network</u></center>\n",
    "\n",
    "(from [Babarasi et Oltvai 2004](https://www.researchgate.net/publication/8910145_Network_Biology_Understanding_The_Cell%27s_Functional_Organization))\n",
    "\n",
    "<div style=\"width: 100%;  padding-top:10px; padding-bottom:10px;border: 3px solid #A0A0A0; text-align: center;background: #EEE3E0;\"> Leveraging this structure can be a possible lead to condense the information of the transcriptome, by trying to identify these clusters and these highly connected genes.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancer\n",
    "\n",
    "During the life of the cell, its DNA can undergo mutations, which, if not corrected, are passed on to its descendants.\n",
    "From cell generation to cell generation, the mutations accumulate until they alter the cell's functioning.\n",
    "One of the possible consequences is that the cell escapes the control of the organism and starts to proliferate in an anarchic manner. The cell then becomes tumorous (or cancerous).\n",
    "\n",
    "It occurs unfortunately rather frequently in the case of the breast cells, which we propose to study here.\n",
    "\n",
    "The expression of genetic information in a cancerous cell is largely modified compared to that of healthy cells, and it is this parameter that interests us here (estimated using the transcriptome).\n",
    "\n",
    "By comparing the transcriptome of many different cancer cells with those of healthy cells, researcher have  [shown](https://www.nature.com/articles/leu201119/) that the alteration of certain gene expressions leads preferentially to cancer :\n",
    "- a gene which is over-expressed in a cancerous cell is called an *[oncogene](https://en.wikipedia.org/wiki/Oncogene)*.\n",
    "- Conversely, a gene that is under-expressed in a cancer cell is called a *[tumour suppressor gene](https://en.wikipedia.org/wiki/Tumor_suppressor)*.\n",
    "\n",
    "Together, these genes are known as the *cancer driver genes*.\n",
    "\n",
    "<div style=\"width: 100%;  padding-top:10px; padding-bottom:10px;border: 3px solid #A0A0A0; text-align: center;background: #EEE3E0;\"> Another way of reducing the high-dimensional transcriptomic data (instead of leveraging the gene network structure), is by identifying such cancer driver genes, and using them to predict the seriousness of the cancer.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival analysis <a name=\"survival_analysis\"></a>\n",
    "\n",
    "#### Censoring of the data\n",
    "\n",
    "*[Survival analysis](https://en.wikipedia.org/wiki/Survival_analysis)* is a branch of statistics designed to model the life or activity time of a living being or device. Widely used in medicine and particularly in oncology, it allows to process data often *[censored](https://en.wikipedia.org/wiki/Censoring_(statistics))* over time. \n",
    "\n",
    "Censored data consist in data for which the observed event (e.g. death for this challenge) is only partially known. As far as oncology is concerned, it is inevitable that patients become out of reach or leave the study before the measured event (death), which leads to *right-censored* data. This means that a lower bound on the patient's life time is known, but not its exact value.\n",
    "\n",
    "Classic statistical tools such as regression are then poorly adapted and may underestimate the lifetimes studied (if they consider that the time at which the patient leave the study is its time of death).\n",
    "\n",
    "#### Computational specificity\n",
    "\n",
    "This is why many specific methods, often derived from classical statistical models, have been developed for survival analysis, and are notably implemented in the [```scikit-survival```](https://scikit-survival.readthedocs.io/en/latest/user_guide/index.html) or [```pysurvival```](https://square.github.io/pysurvival/) Python libraries.\n",
    "\n",
    "They require specific datasets, including a supplementary vector compared to regular datasets. Indeed, in addition to a classical dataset with $n$ samples, containing the $d$-dimensional features in the matrix $X \\in \\mathbb{R}^{nd}$ and the associated target values in the vector $y \\in \\mathbb{R}^{n}$, a survival dataset includes a third information : $E \\in \\mathbb{R}^{n}$. This vector contains a boolean indicator for each sample, telling if the event has occured (e.g. the patient $i$ is dead $\\iff E_{i} =1$) or not (e.g. the patient $j$ is censored $\\iff E_{j} = 0$).\n",
    "\n",
    "To summarize, for a dataset containing $n$ samples, whose features have $d$ dimensions :\n",
    "\n",
    "| Data | Regular dataset | Survival dataset |\n",
    "| :- | :-: | :-: |\n",
    "| Features | $X \\in \\mathbb{R}^{nd}$ | $X \\in \\mathbb{R}^{nd}$ |\n",
    "| Target   | $y \\in \\mathbb{R}^{n}$ | $y \\in \\mathbb{R}^{n}$ |\n",
    "| Events   | $\\emptyset$ | $E \\in \\mathbb{R}^{n}$ |\n",
    "\n",
    "#### Advise for the challenge\n",
    "\n",
    "You will be given $X_{\\texttt{train}}$, $y_{\\texttt{train}}$ and $E_{\\texttt{train}}$, and you will have to build a regressor able to accurately predict $y_{\\texttt{test}}$ given $X_{\\texttt{test}}$. (You will not be given $E_{\\texttt{test}}$, which is logical : imagine a patient asks you to predict how long she will survive given the transcriptome of her tumour; you wouldn't have the information $E$ regarding her.).\n",
    "\n",
    "You can choose to ignore the information given by $E_{\\texttt{train}}$ (or include it only in the design of the features). If you do so, you can then create a classic regressor, such as those provided by ```scikit-learn```, which uses only $X_{\\texttt{train}}$ and $y_{\\texttt{train}}$.\n",
    "\n",
    "Otherwise, you can choose to directly use a survival regressor as those provided by the precedently cited Python libraries, which takes into account $E_{\\texttt{train}}$. *This is the recommended p\n",
    "\n",
    "<p style=\"color:#C82801\";><b>The aim of this challenge is not so much to develop a new model, <i>although this is possible for the participants who wish to do so,</i> but rather to overcome the curse of the dimension by <u>designing relevant features to reduce the dimension</u>, and using them with existant survival models.\n",
    "</b></p>\n",
    "\n",
    "Throughout this notebook, we will present some basic relevant statistical techniques for survival analysis that might be useful for the challenge. However, the real difficulty of this challenge does not lie in the design of a sophisticated survival model, but in that of reducing the dimension, for which no knowledge of survival analysis is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration <a name=\"data_exploration\"></a>\n",
    "\n",
    "### Python requirements <a name=\"python_requirements\"></a>\n",
    "\n",
    "In order to collect and analyse the data, the following Python libraries are required :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Generic requirements\n",
      "numpy\n",
      "pandas\n",
      "scipy\n",
      "sklearn\n",
      "\n",
      "# Gather the data\n",
      "mygene\n",
      "xenaPython\n",
      "\n",
      "# Survival analysis\n",
      "scikit-survival\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('requirements.txt', 'r') as requirements:\n",
    "    print(requirements.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which can be installed (under Linux) with :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather the data <a name=\"getting_data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you have to download the data, by running :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: ligne 1: python : commande introuvable\r\n"
     ]
    }
   ],
   "source": [
    "!python download_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rampwf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b9c382e5e2fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mproblem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_test_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_test_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Datascience/Datacamp/Cancer-survival-challenge/problem.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Ramp imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mrampwf\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrampwf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore_types\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseScoreType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrampwf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkflows\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSKLearnPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rampwf'"
     ]
    }
   ],
   "source": [
    "from problem import get_train_data, get_test_data\n",
    "X_train, y_train = get_train_data()\n",
    "X_test, y_test = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : Although 55511 genes were listed, the data for \"only\" 37496 genes are available.\n",
    "\n",
    "We also note the two last columns :\n",
    "\n",
    "+ ```death``` can take two values : \n",
    "    + ```1``` means the patient died\n",
    "    + ```0``` means the patient was censored\n",
    "+ ```time``` is equal to the time of the event (either death or censoring)\n",
    "\n",
    "Thus, instead of having two variables ```X``` and ```y```, we have three variables ```X```, ```y``` and ```E``` constructed as follows :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[:,0].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival analysis for a gene\n",
    "\n",
    "#### A few definitions <a name=\"definitions\"></a>\n",
    "\n",
    "To make this introduction to survival analysis more readable, we can assume that time is a discrete variable i.e $t \\in \\{1,...,n\\}$ with $n \\in \\mathbb{N}$.\n",
    "\n",
    "Let $ \\tau \\geq 0$ be the random variable giving the time before a relevant event happen. Usually, survival prediction aims to estimate the *survival function* $S$ which define  $\\tau$, where $S$ is defined by :\n",
    "$ {\\displaystyle S(t)=\\mathrm {Prob} (\\tau >t)}$\n",
    "\n",
    "However, we limit ourselves to predicting the life time of the patients. (It could for example be estimated from such a survival function by setting a threshold $d$ below which we consider that the patient dies. The life time of a patient would then be $argmax_{t} \\text{  s.t.  } S(t) > d$)\n",
    "\n",
    "We also make the assumption that the censors (the patients leaving the study) are non-informative (e.g. they don't have anything to do with the subject studied, e.g. can be considered random).\n",
    "\n",
    "#### The Kaplan-Meier estimator <a name=\"kaplan_meier\"></a>\n",
    "\n",
    "The Kaplan-Meier estimator is one of the most classic tool of survival analysis. Indeed, it is a non-parametric estimator (eg.g it does not make any assumption on the data distribution accross time), quite simple to implement, and it uses the information from the censored lineage.\n",
    "The Kaplan-Meier estimator can be written : \n",
    "${\\displaystyle {\\hat{S}}(t)=\\prod_{i:t_{i}\\leq t}\\left(1-{\\frac{d_{i}}{n_{i}}}\\right)}$. \n",
    " \n",
    "where $t_i$ is a time when at least one event happened, $d_i$ the number of event at time ${\\displaystyle t_{i}}$ and $n_i$ the individuals still in the study (neither dead nor censored) at time $t_i$.\n",
    "\n",
    "Intuitively, the product to compute ${\\hat{S}}(t)$ is on all time less than t, and thus allows to take into account all patients, even those *censored* before time $t$ because they got out of the reach of the study.\n",
    "\n",
    "\n",
    "Visually, a representation of a Kaplan-Meier estimator gives a decreasing staircase function, depicting an estimation of the survival function $S(t)$ ($S$ is supposed to be constant between two events).\n",
    "\n",
    "By allowing to estimate the survival function of a sample, the Kaplan-Meier estimator enables the comparison between two different samples (for instance between a medicated group and a control group). But to this aim, other methods can be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The log-rank test <a name=\"log_rank_test\"></a>\n",
    "\n",
    "The log-rank test is a non-parametric hypothesis test allowing to compare the survival functions of two different populations. In practice, it's used to quantify the survival differences between two populations which can be observed by plotting their Kaplan-Meier estimation.\n",
    "\n",
    "Indeed, the log-rank test also allows to take into account censored lineage without loosing to much information.\n",
    "\n",
    "A few notations :\n",
    "\n",
    "For each time j, we write $N_{{1j}}$ and $N_{{2j}}$ the number of patients from each group in each group still studied at time j, and $N_{j}=N_{{1j}}+N_{{2j}}$. Let's write $O_{{1j}}$ and $O_{{2j}}$ the number of events (e.g deaths) in each group happening at time j, and similarly $O_{j}=O_{{1j}}+O_{{2j}}$.\n",
    "\n",
    "The idea of the log-rank test is to test the null hypothesis \"the two population have the same survival function\".\n",
    "\n",
    "Under this hypothesis, $O_{1j}$ follows an hypergeometric law with parameters $N_j$, $N_{1j}$ and $O_j$, and thus with expectation $E_{1j}$ and variance $V_j$.\n",
    "\n",
    "The convergence being garanteed by the Central Limit Theorem (Lyapunov version), we can define the random variable $${\\displaystyle Z={\\frac {\\sum _{j=1}^{J}(O_{1j}-E_{1j})}{\\sqrt {\\sum _{j=1}^{J}V_{j}}}}{\\xrightarrow {\\mathbb{L}}\\ N(0,1).}}$$\n",
    "\n",
    "It is then easy to quantify the ressemblance between the law of $Z$ and a standard normal law to see if the null hypothesis can be rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first feature : the correlation with the survival time\n",
    "\n",
    "A first idea would be to select only those genes that are most correlated, in absolute terms, with survival time. To do this, uncensored patients are filtered out and the correlation with survival time of each gene from the remaining patients is computed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_uncensored = X_train[y_train[:,0]==1]\n",
    "y_uncensored = y_train[y_train[:,0]==1]\n",
    "print('They are %i uncensored patients' % len(X_uncensored))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the correlation for those patients.\n",
    "\n",
    "Pearson correlation relies on the assumption that the variable are a normally distributed, which is clearly not the case here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,6))\n",
    "sns.histplot(y_uncensored[:,1], ax=ax1, stat='probability',bins=25)\n",
    "ax1.set_title('Distribution of the time to death')\n",
    "ax1.set_xlabel('Time to death')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "log_time = np.log(y_uncensored[:,1])\n",
    "sns.histplot(log_time, ax=ax2, stat='probability', color='r', bins=25)\n",
    "ax2.set_title('Distribution of the logarithm of the time to death')\n",
    "ax2.set_xlabel('Time to death (log)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logarithmic transformation slightly improves the time distribution and makes it a little more normal.\n",
    "Let's compute the correlation of the expression of each gene with the time to death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.stats as stats\n",
    "\n",
    "df_log_time = pd.Series(log_time)\n",
    "\n",
    "# First we remove the mean and we scale each gene expression data to unit variance\n",
    "scaler = StandardScaler()\n",
    "scaled_X_train = scaler.fit_transform(X_uncensored)\n",
    "scaled_df = pd.DataFrame(data=scaled_X_train)\n",
    "\n",
    "# Then we compute the correlations\n",
    "correlations = scaled_df.corrwith(df_log_time)\n",
    "np_correlations = correlations.dropna().to_numpy().reshape(-1,1)\n",
    "\n",
    "sns.histplot(np_correlations, stat='probability', color='green')\n",
    "plt.title('Distribution of the correlation between the genes expression and the time before death')\n",
    "plt.xlabel('Correlation value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, the distribution of correlations seems close to the normal law... The comparison between the two laws can be refined by means of a Q-Q plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_correlations = scaler.fit_transform(np_correlations)\n",
    "stats.probplot(scaled_correlations.flatten(), dist=\"norm\",plot=plt)\n",
    "plt.title('QQ-plot between the normal distribution and the correlation distribution.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "corr_r, p_values = [], []\n",
    "print(log_time.shape)\n",
    "for gene in scaled_X_train.T:\n",
    "    r, p_val = pearsonr(gene, log_time)\n",
    "    corr_r.append(r)\n",
    "    p_values.append(p_val)\n",
    "sns.histplot(p_values, stat='probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(corr_r, stat='probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(p_values, dist=\"uniform\",plot=plt)\n",
    "plt.title('QQ-plot between the uniform distribution and the p-values distribution.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,6))\n",
    "sns.histplot(y_train[:,1], ax=ax1, stat='probability',bins=30)\n",
    "ax1.set_title('Distribution of the time to death')\n",
    "ax1.set_xlabel('Time to death')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "log_time = np.log(y_train[:,1])\n",
    "sns.histplot(log_time, ax=ax2, stat='probability', color='r', bins=30)\n",
    "ax2.set_title('Distribution of the logarithm of the time to death')\n",
    "ax2.set_xlabel('Time to death (log)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_time = pd.Series(log_time)\n",
    "\n",
    "# First we remove the mean and we scale each gene expression data to unit variance\n",
    "scaler = StandardScaler()\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_df = pd.DataFrame(data=scaled_X_train)\n",
    "\n",
    "# Then we compute the correlations\n",
    "correlations = scaled_df.corrwith(df_log_time)\n",
    "np_correlations = correlations.dropna().to_numpy().reshape(-1,1)\n",
    "\n",
    "sns.histplot(np_correlations, stat='probability', color='green')\n",
    "plt.title('Distribution of the correlation between the genes expression and the time before death')\n",
    "plt.xlabel('Correlation value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np_correlations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_correlations = scaler.fit_transform(np_correlations)\n",
    "stats.probplot(scaled_correlations.flatten(), dist=\"norm\",plot=plt)\n",
    "plt.title('QQ-plot between the normal distribution and the correlation distribution.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_r, p_values = [], []\n",
    "for gene in scaled_X_train.T:\n",
    "    r, p_val = pearsonr(gene, log_time)\n",
    "    corr_r.append(r)\n",
    "    p_values.append(p_val)\n",
    "sns.histplot(p_values, stat='probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.probplot(p_values, dist=\"uniform\",plot=plt)\n",
    "plt.title('QQ-plot between the uniform distribution and the p-values distribution.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some ideas :\n",
    "+ Study the gene expression distributions, those who are very expressed, those who aren't...\n",
    "+ Study the correlations among the gene expression\n",
    "+ Study the difference of gene expression (those with the biggest difference for example) between alive patients and dead patients\n",
    " \n",
    " ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building training and testing sets\n",
    "N = len(df1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "index_train, index_test = train_test_split( range(N), test_size = 0.2, random_state=10)\n",
    "data_train = df1.loc[index_train].reset_index( drop = True )\n",
    "data_test  = df1.loc[index_test].reset_index( drop = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train['death'].value_counts())\n",
    "print(data_test['death'].value_counts())\n",
    "E_train, E_test = data_train.pop('death'), data_test.pop('death')\n",
    "y_train, y_test = data_train.pop('time'), data_test.pop('time')\n",
    "X_train, X_test = data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysurvival.models.survival_forest import ConditionalSurvivalForestModel\n",
    "from pysurvival.models.semi_parametric import CoxPHModel\n",
    "from pysurvival.utils.metrics import concordance_index\n",
    "from pysurvival.utils.metrics import integrated_brier_score as ib_score\n",
    "from pysurvival.utils.display import integrated_brier_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X_train)\n",
    "X_train_transformed = pca.transform(X_train)\n",
    "X_test_transformed = pca.transform(X_test)\n",
    "\n",
    "#lr = LinearRegression().fit(X_train, y_train)\n",
    "#c_index = concordance_index(lr, X_test_transformed, y_test, E_test)\n",
    "#print('C-index: {:.2f}'.format(c_index)) #0.1\n",
    "#ibs = ib_score(lr, X_test_transformed, y_test, E_test, t_max=max_time)\n",
    "#print(ibs)\n",
    "\n",
    "coxph = CoxPHModel()\n",
    "coxph.fit(X_train_transformed, y_train, E_train, lr=0.1, l2_reg=1e-1, max_iter=1000)\n",
    "c_index = concordance_index(coxph, X_test_transformed, y_test, E_test)\n",
    "print('C-index: {:.2f}'.format(c_index)) #0.1\n",
    "ibs = ib_score(coxph, X_test_transformed, y_test, E_test, t_max=max_time)\n",
    "print('IBS: {:.9f}'.format(ibs)) \n",
    "integrated_brier_score(coxph, X_test_transformed, y_test, E_test, t_max=max_time, figure_size=(20, 6.5))\n",
    "\n",
    "# Fitting the model\n",
    "csf = ConditionalSurvivalForestModel(num_trees=10)\n",
    "csf.fit(X_train_transformed, y_train, E_train, max_features='sqrt')\n",
    "\n",
    "from pysurvival.utils.display import display_loss_values\n",
    "#display_loss_values(csf, figure_size=(7, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysurvival.models.multi_task import LinearMultiTaskModel\n",
    "import numpy as np\n",
    "lmtm = LinearMultiTaskModel(bins=200)\n",
    "lmtm.fit(X_train_transformed, y_train, E_train, lr=0.0001, l2_reg=1e-1,num_epochs=1000)\n",
    "print(lmtm.times)\n",
    "c_index = concordance_index(lmtm, X_test_transformed, y_test, E_test)\n",
    "print('C-index: {:.2f}'.format(c_index)) #0.1\n",
    "ibs = ib_score(lmtm, X_test_transformed, y_test, E_test, t_max=max_time)\n",
    "print('IBS: {:.9f}'.format(ibs)) \n",
    "integrated_brier_score(lmtm, X_test_transformed, y_test, E_test, t_max=max_time, figure_size=(20, 6.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = lmtm.predict_survival(X_test_transformed)\n",
    "ibs = ib_score(i, X_test_transformed, y_test, E_test, t_max=max_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_melt = pd.concat([E_train, y_train], axis=1)\n",
    "print(df_to_melt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb as pdb\n",
    "def to_structured_array(E_df, y_df):\n",
    "    E = E_df.to_numpy().astype(bool)\n",
    "    y = y_df.to_numpy()\n",
    "    w = np.column_stack((E, y))\n",
    "    w = w.ravel().view([('event', w[0].dtype), ('time', y.dtype)]).astype('bool, <i8')\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sksurv.metrics import concordance_index_ipcw\n",
    "risk = lmtm.predict_risk(X_test_transformed)\n",
    "s_train, s_test = to_structured_array(E_train, y_train), to_structured_array(E_test, y_test)\n",
    "concordance_index_ipcw(s_train, s_test, risk)\n",
    "test_risk = - y_test + 4500\n",
    "print(concordance_index_ipcw(s_train, s_test, test_risk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysurvival.utils.metrics import concordance_index\n",
    "c_index = concordance_index(csf, X_test_transformed, y_test, E_test)\n",
    "print('C-index: {:.2f}'.format(c_index)) #0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysurvival.utils.display import integrated_brier_score\n",
    "ibs = integrated_brier_score(csf, X_test_transformed, y_test, E_test, t_max=10000,\n",
    "                       figure_size=(20, 6.5) )\n",
    "print('IBS: {:.9f}'.format(ibs)) #0.92"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model <a name=\"baseline_model\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A simple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Cox regression<a name=\"cox_regression\"></a>\n",
    "\n",
    "The Cox regression is the equivalent of the logistic regression for survival analysis.\n",
    "Let's define the *hazard rate*\n",
    "$$\\lambda(t)=\\lim _{d t \\rightarrow 0} \\frac{\\operatorname{Pr}(t \\leq T<t+d t)}{d t \\cdot S(t)}=-\\frac{S^{\\prime}(t)}{S(t)}$$\n",
    "defined as the probability of dying at a certain time t given the fact that the patient survived until time t.\n",
    "The Cox regression belong to the class of *proportional hazard models*, that is models which make the assumption that each unit increase for a covariate multiply the hazard rate of the patient by a fixed amount. For instance, taking a certain drug could halve the probability of dying from cancer at each time t.\n",
    "\n",
    "This idea is formalized this way : \n",
    "\n",
    "Let $X_{i}=\\left\\{X_{i 1}, \\ldots X_{i p}\\right\\}$ be the covariate values for a certain patient i.\n",
    "Then we model the hazard rate $\\lambda$ by \n",
    "<p style=\"color:#FF0000\";>$${\\lambda\\left(t | X_{i}\\right)=\\lambda_{0}(t) \\exp \\left(\\beta_{1} X_{i 1}+\\cdots+\\beta_{p} X_{i p}\\right)=\\lambda_{0}(t) \\exp \\left(X_{i} \\cdot \\beta\\right)}$$</p>.\n",
    "\n",
    "Let's  note that only the base hazard rate is time-dependant : the effect of each covariate on the *hazard rate* is assumed to be the same across time.\n",
    "\n",
    "This property is the main \"trick\" of Cox regression : it allows to get rid of the hard-to-estimate hazard rate by taking a ratio. To be more precise, the likelihood of an event to be observed on patient i at time $Y_i$ is $$L_{i}(\\beta)=\\frac{\\lambda\\left(Y_{i} | X_{i}\\right)}{\\sum_{j : Y_{j} \\geq Y_{i}} \\lambda\\left(Y_{i} | X_{j}\\right)}=\\frac{\\lambda_{0}\\left(Y_{i}\\right) \\theta_{i}}{\\sum_{j : Y_{j} \\geq Y_{i}} \\lambda_{0}\\left(Y_{i}\\right) \\theta_{j}}=\\frac{\\theta_{i}}{\\sum_{j : Y_{j} \\geq Y_{i}} \\theta_{j}},$$ with $\\theta_{j}=\\exp \\left(X_{j} \\cdot\\beta\\right)$ and the summation being on all the patient still alive and in the study at time $Y_i$.\n",
    "\n",
    "Skipping the most technical details, the idea is then to model all patients as independant and to maximize the product likekihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric\n",
    "\n",
    "We will use the concordance index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission <a name=\"submission\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
