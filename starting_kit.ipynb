{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Breast cancer survival prediction</center>\n",
    "\n",
    "*Authors: Alexandre Marquis, Michael Resplandy, Gabriel Faivre, Hugo Stubler, Stéphane Béreux*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "    1. [Basic cellular biology concepts](#base_biology)\n",
    "        1. [Genetic information](#genetic_information)\n",
    "        2. [Transcriptome](#transcriptome)\n",
    "        3. [Gene network](#gene_network)\n",
    "    2. [Cancer](#cancer)\n",
    "    1. [Survival analysis](#survival_analysis)\n",
    "        1. [Censoring of the data](#censoring_of_the_data)\n",
    "        2. [Computational specificity](#computational_specificity)\n",
    "        3. [Advise for the challenge](#advise_for_the_challenge)\n",
    "2. [Data exploration](#data_exploration)\n",
    "    1. [Python requirements](#python_requirements)\n",
    "    2. [Gather the data](#getting_data)\n",
    "3. [Overview of the data](#overview_of_the_data)\n",
    "4. [Survival analysis for a gene](#survival_analysis_for_a_gene)\n",
    "    1. [A few definitions](#definitions)\n",
    "    2. [The Kaplan-Meier estimator](#kaplan_meier)\n",
    "    3. [The log-rank test](#log_rank_test)\n",
    "5. [Feature design](#feature_design)\n",
    "    1. [Identification of the cancer driver genes](#identification_of_the_cancer_driver_genes)\n",
    "    2. [Considering the whole dataset](#considering_the_whole_dataset)\n",
    "    3. [Comparing with the Cancer Gene Census database](#comparing_with_the_cancer_gene_census_database)\n",
    "    4. [Leveraging the gene network topology](#leveraging_the_gene_network_topology)\n",
    "6. [Metric](#metric)\n",
    "7. [Baseline model](#baseline_model)\n",
    "    1. [The Cox regression](#cox_regression)\n",
    "5. [Submission](#submission)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "\n",
    "Breast cancer is one of the most common cancers and the second leading cause of cancer death among women in the United States. One in nine women will be diagnosed with breast cancer in her lifetime ([INCa avril 2016](https://www.ligue-cancer.net/article/26094_cancer-du-sein)). Approximately 70% of breast cancer patients are inoperable because of advanced tumor growth or bone metastasis [Min Tao et al.](https://pubmed.ncbi.nlm.nih.gov/21512769/). \n",
    "\n",
    "It is therefore crucial to be able to accurately diagnose the disease, and to better understand the aggravating factors. Here we propose to predict the survival based on the genetic factors of the tumour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic cellular biology concepts <a name=\"base_biology\"></a>\n",
    "\n",
    "The following explanation is deliberately simplified, and does not get bogged down in unnecessary biological details that are irrelevant to the challenge. \n",
    "It simply aims to present a schematic vision of the functioning of the majority of human cells*, which justifies the approach adopted in this challenge, and thus gives possible leads for the participants.\n",
    "\n",
    "\\**except in particular degenerated cells such as red blood cells...*\n",
    "\n",
    "#### Genetic information <a name=\"genetic_information\"></a>\n",
    "\n",
    "The human being is made up of approximately $10^{13}$ cells. These cells are not immutable: on the contrary they are themselves living entities, which are born, live and die.\n",
    "\n",
    "Each human cell has its genetic information contained in [DNA](https://en.wikipedia.org/wiki/DNA#Biological_functions), a molecule with excellent storage capacities thanks to its stability and its faithful transmission from the mother cell to the daughter cells. This information is divided into genes.\n",
    "\n",
    "During its life, the cell is led to express part of this information (e.g. part of the genes), by translating it into molecular tools. \n",
    "The modalities of expression differ according to the genes, but for the vast majority, the beginning of the expression process is the same. \n",
    "\n",
    "#### Transcriptome <a name=\"transcriptome\"></a>\n",
    "\n",
    "To be expressed, each gene is first [transcribed](https://en.wikipedia.org/wiki/Transcription_(biology)) into multiple copies of itself, in the form of [RNA](https://en.wikipedia.org/wiki/RNA#Comparison_with_DNA) molecules (a molecule close to DNA). Each copy of RNA contains the same genetic information as the original gene, but in a more reactive (and therefore more ephemeral) form than DNA. This makes it more accessible for expression.\n",
    "\n",
    "![central-dogma.png](img/central-dogma.png)\n",
    "\n",
    "<center><u>Pipeline of the expression of the genetic information : transcription into RNA and translation into proteins</u></center>\n",
    "\n",
    "*(from https://www.atdbio.com/)*\n",
    "\n",
    "*(The further expression of the genetic information is beyond the scope of this challenge, but essentially consists of two possibilities: either the RNA molecule is directly used as a molecular tool, or an additional step takes place ([translation](https://en.wikipedia.org/wiki/Translation_(biology))) which translates the RNA molecule into a protein.)*\n",
    "\n",
    "The expression of a gene is therefore controlled by the number of RNA copies produced for each gene. The set of RNA molecules in a cell is called the [transcriptome](https://en.wikipedia.org/wiki/Transcriptome). \n",
    "Here we (classically) propose to use the transcriptome as a proxy to infer the expression of each gene.\n",
    "\n",
    "As you will see in the following part, the main difficulty of this challenge will be to reduce the dimension of the very high-dimensional transcriptomic data (expression data for more than 35000 genes), in order to accurately predict the survival time of breast cancer patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Genes network  <a name=\"gene_network\"></a>\n",
    "\n",
    "Genes can interact with each other, which mean that some genes will induce or inhibits the expression of other genes. [Inferring these relationships](https://en.wikipedia.org/wiki/Gene_regulatory_network) has been an active area of research, and the [results](https://www.researchgate.net/publication/8910145_Network_Biology_Understanding_The_Cell%27s_Functional_Organization) tend to show that this network is a [hierarchical scale-free network](https://en.wikipedia.org/wiki/Hierarchical_network_model), hence a rather sparse network containing only some highly connected nodes, surrounded by local dense clusters.\n",
    "\n",
    "![hierarchical](img/hierarchical.jpg)\n",
    "<center><u>Example of hierarchical scale-free network</u></center>\n",
    "\n",
    "(from [Babarasi et Oltvai 2004](https://www.researchgate.net/publication/8910145_Network_Biology_Understanding_The_Cell%27s_Functional_Organization))\n",
    "\n",
    "<div style=\"width: 100%;  padding-top:10px; padding-bottom:10px;border: 3px solid #A0A0A0; text-align: center;background: #EEE3E0;\"> Leveraging this structure can be a possible lead to condense the information of the transcriptome, by trying to identify these clusters and these highly connected genes.</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cancer  <a name=\"cancer\"></a>\n",
    "\n",
    "During the life of the cell, its DNA can undergo mutations, which, if not corrected, are passed on to its descendants.\n",
    "From cell generation to cell generation, the mutations accumulate until they alter the cell's functioning.\n",
    "One of the possible consequences is that the cell escapes the control of the organism and starts to proliferate in an anarchic manner. The cell then becomes tumorous (or cancerous).\n",
    "\n",
    "It occurs unfortunately rather frequently in the case of the breast cells, which we propose to study here.\n",
    "\n",
    "The expression of genetic information in a cancerous cell is largely modified compared to that of healthy cells, and it is this parameter that interests us here (estimated using the transcriptome).\n",
    "\n",
    "By comparing the transcriptome of many different cancer cells with those of healthy cells, researcher have  [shown](https://www.nature.com/articles/leu201119/) that the alteration of certain gene expressions leads preferentially to cancer :\n",
    "- a gene which is over-expressed in a cancerous cell is called an *[oncogene](https://en.wikipedia.org/wiki/Oncogene)*.\n",
    "- Conversely, a gene that is under-expressed in a cancer cell is called a *[tumour suppressor gene](https://en.wikipedia.org/wiki/Tumor_suppressor)*.\n",
    "\n",
    "Together, these genes are known as the *cancer driver genes*.\n",
    "\n",
    "<div style=\"width: 100%;  padding-top:10px; padding-bottom:10px;border: 3px solid #A0A0A0; text-align: center;background: #EEE3E0;\"> Another way of reducing the high-dimensional transcriptomic data (instead of leveraging the gene network structure), is by identifying such cancer driver genes, and using them to predict the seriousness of the cancer.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival analysis <a name=\"survival_analysis\"></a>\n",
    "\n",
    "#### Censoring of the data <a name=\"censoring_of_the_data\"></a>\n",
    "\n",
    "*[Survival analysis](https://en.wikipedia.org/wiki/Survival_analysis)* is a branch of statistics designed to model the life or activity time of a living being or device. Widely used in medicine and particularly in oncology, it allows to process data often *[censored](https://en.wikipedia.org/wiki/Censoring_(statistics))* over time. \n",
    "\n",
    "Censored data consist in data for which the observed event (e.g. death for this challenge) is only partially known. As far as oncology is concerned, it is inevitable that patients become out of reach or leave the study before the measured event (death), which leads to *right-censored* data. This means that a lower bound on the patient's life time is known, but not its exact value.\n",
    "\n",
    "Usually, we make the assumption that \n",
    "\n",
    "Classic statistical tools such as regression are then poorly adapted and may underestimate the lifetimes studied (if they consider that the time at which the patient leave the study is its time of death).\n",
    "\n",
    "#### Computational specificity <a name=\"computational_specificity\"></a>\n",
    "\n",
    "This is why many specific methods, often derived from classical statistical models, have been developed for survival analysis, and are notably implemented in the [```scikit-survival```](https://scikit-survival.readthedocs.io/en/latest/user_guide/index.html) or [```pysurvival```](https://square.github.io/pysurvival/) Python libraries.\n",
    "\n",
    "They require specific datasets, including a supplementary vector compared to regular datasets. Indeed, in addition to a classical dataset with $n$ samples, containing the $d$-dimensional features in the matrix $X \\in \\mathbb{R}^{nd}$ and the associated target values in the vector $y \\in \\mathbb{R}^{n}$, a survival dataset includes a third information : $E \\in \\mathbb{R}^{n}$. This vector contains a boolean indicator for each sample, telling if the event has occured (e.g. the patient $i$ is dead $\\iff E_{i} =1$) or not (e.g. the patient $j$ is censored $\\iff E_{j} = 0$).\n",
    "\n",
    "To summarize, for a dataset containing $n$ samples, whose features are $d$-dimensional :\n",
    "\n",
    "| Data | Regular dataset | Survival dataset |\n",
    "| :- | :-: | :-: |\n",
    "| Features | $X \\in \\mathbb{R}^{nd}$ | $X \\in \\mathbb{R}^{nd}$ |\n",
    "| Target   | $y \\in \\mathbb{R}^{n}$ | $y \\in \\mathbb{R}^{n}$ |\n",
    "| Events   | $\\emptyset$ | $E \\in \\mathbb{R}^{n}$ |\n",
    "\n",
    "#### Advise for the challenge <a name=\"advise_for_the_challenge\"></a>\n",
    "\n",
    "You will be given $X_{\\texttt{train}}$, $y_{\\texttt{train}}$ and $E_{\\texttt{train}}$, and you will have to build a regressor able to accurately predict $y_{\\texttt{test}}$ given $X_{\\texttt{test}}$. (You will not be given $E_{\\texttt{test}}$, which is logical : imagine a patient asks you to predict how long she will survive given the transcriptome of her tumour; you wouldn't have the information $E$ regarding her.).\n",
    "\n",
    "You can choose to ignore the information given by $E_{\\texttt{train}}$ (or include it only in the design of the features). If you do so, you can then create a classic regressor, such as those provided by [```scikit-learn```](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning), which uses only $X_{\\texttt{train}}$ and $y_{\\texttt{train}}$.\n",
    "\n",
    "Otherwise, you can choose to directly use a survival regressor as those provided by the precedently cited Python libraries, which takes into account $E_{\\texttt{train}}$. *This is the recommended procedure*.\n",
    "\n",
    "<p style=\"color:#C82801\";><b>The aim of this challenge is not so much to develop a new model, <i>although this is possible for the participants who wish to do so,</i> but rather to overcome the curse of the dimension by <u>designing relevant features to reduce the dimension</u>, and using them with existant survival models.\n",
    "</b></p>\n",
    "\n",
    "Throughout this notebook, we will present some basic relevant statistical techniques for survival analysis that might be useful for the challenge. However, the real difficulty of this challenge does not lie in the design of a sophisticated survival model, but in that of reducing the dimension of the transcriptomic data, for which no knowledge of survival analysis is required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration <a name=\"data_exploration\"></a>\n",
    "\n",
    "#### Python requirements <a name=\"python_requirements\"></a>\n",
    "\n",
    "In order to collect and analyse the data, the following Python libraries are required :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Generic requirements\n",
      "numpy\n",
      "pandas\n",
      "scipy\n",
      "sklearn\n",
      "seaborn\n",
      "matplotlib\n",
      "tqdm\n",
      "\n",
      "# Gather the data\n",
      "mygene\n",
      "xenaPython\n",
      "\n",
      "# Survival analysis\n",
      "scikit-survival\n",
      "\n",
      "# Requierements for RAMP Studio\n",
      "ramp-workflow\n",
      "\n",
      "# Graph library\n",
      "networkx\n"
     ]
    }
   ],
   "source": [
    "with open('requirements.txt', 'r') as requirements:\n",
    "    print(requirements.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which can be installed (under Linux) with :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import probplot, pearsonr\n",
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gather the data <a name=\"getting_data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you have to download the data, by running :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/train were already downloaded.\r\n",
      "data/test were already downloaded.\r\n"
     ]
    }
   ],
   "source": [
    "!python download_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem import get_train_data, get_test_data\n",
    "X_train, E_y_train = get_train_data()\n",
    "X_test, E_y_test = get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E$ and $y$ are stored together. As only $y$ has to be predicted, let's separate ```E_y_train``` into ```E_train``` and ```y_train```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_train, y_train = E_y_train[:,0], E_y_train[:,1]\n",
    "E_test, y_test = E_y_test[:,0], E_y_test[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $X$, $E$ and $y$, we are ready to take a closer look at the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the data <a name=\"overview_of_the_data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The training dataset contains %i patients, %.2f percents of whom are censored.' % (len(y_train), 100 * (1 - E_train.sum() / len(E_train))))\n",
    "print('The testing dataset contains %i patients, %.2f percents of whom are censored.' % (len(y_test),  100 * (1 - E_test.sum() / len(E_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This high rate of right-censored data makes it necessary to use the information provided by the censored samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the survival data. First we look only at the patients for whom we know the time of death, i.e. that are not censored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_uncensored = E_train == 1\n",
    "\n",
    "print('They are %i uncensored patients' % sum(is_uncensored))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the survival time for these patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame()\n",
    "data_df['survival'] = y_train\n",
    "r = range(np.max(data_df['survival']))\n",
    "survival = [len(data_df['survival'].loc[data_df['survival'] > year]) for year in r]\n",
    "plt.plot(r, survival)\n",
    "plt.xlabel(\"Survival Time (days)\")\n",
    "plt.ylabel(\"Number of individuals\")\n",
    "plt.title(\"Survival Time for Breast Cancer\")\n",
    "plt.show()\n",
    "\n",
    "print('The survival time is included between %.0f and %.0f days.' % (min(y_train), max(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many gene expression data are available for these patients ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The features contain the expression data for %i genes.' % len(X_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is very high-dimensional : ```d``` $> 35$```n``` : it will obviously be necessary to drastically reduce the dimension of the features.\n",
    "\n",
    "Let's take a closer look at the expression data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_x = X_train.mean()\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20,6))\n",
    "\n",
    "ax1.set_title('Distribution the mean gene expression')\n",
    "ax1.set_xlabel('mean gene expression')\n",
    "sns.histplot(mean_x, ax=ax1)\n",
    "\n",
    "\n",
    "ax2.set_title('Boxplot of the mean gene expression')\n",
    "ax2.set_xlabel('mean gene expression')\n",
    "sns.boxplot(mean_x, ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that many genes have almost zero expression (first column of the histogram). Let's investigate this column: maybe we can drop some of this data, and start to reduce the size of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unexpressed_columns = X_train.columns[(mean_x == 0.).to_numpy()]\n",
    "unexpressed_columns = [column.split('\\n')[0] for column in unexpressed_columns]\n",
    "print('We drop %i columns of genes which are never expressed.' % len(unexpressed_columns))\n",
    "X_train.drop(columns=unexpressed_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survival analysis for a gene <a name=\"survival_analysis_for_a_gene\"></a>\n",
    "\n",
    "We will now present some tools for gene by gene survival analysis. In particular, the two tools that will be presented can be used to identify genes that particularly discriminate between patients, and use them as a feature for regression.\n",
    "\n",
    "This list does not pretend to be exhaustive, but only to give some possible leads.\n",
    "\n",
    "We will illustrate these tools with the gene [```MYC```](https://en.wikipedia.org/wiki/Myc), a well-known oncogene.\n",
    "\n",
    "#### A few definitions <a name=\"definitions\"></a>\n",
    "\n",
    "To make this introduction to survival analysis more readable, we can assume that time is a discrete variable i.e $t \\in \\{1,...,n\\}$ with $n \\in \\mathbb{N}$.\n",
    "\n",
    "Let $ \\tau \\geq 0$ be the random variable giving the time before a relevant event happen. Usually, survival prediction aims to estimate the *survival function* $S$ which define  $\\tau$, where $S$ is defined by :\n",
    "$ {\\displaystyle S(t)=\\mathrm {Prob} (\\tau >t)}$\n",
    "\n",
    "However, we limit ourselves to predicting the life time of the patients. (It could for example be estimated from such a survival function by setting a threshold $d$ below which we consider that the patient dies. The life time of a patient would then be $argmax_{t} \\text{  s.t.  } S(t) > d$)\n",
    "\n",
    "We also make the (*very strong*) assumption that the censors (the patients leaving the study) are non-informative (e.g. they don't have anything to do with the subject studied, in other words they can be considered random)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Kaplan-Meier estimator <a name=\"kaplan_meier\"></a>\n",
    "\n",
    "The Kaplan-Meier estimator is one of the most classic tool of survival analysis. Indeed, it is a non-parametric estimator (eg.g it does not make any assumption on the data distribution accross time), quite simple to implement, and it uses the information from the censored lineage.\n",
    "The Kaplan-Meier estimator can be written : \n",
    "${\\displaystyle {\\hat{S}}(t)=\\prod_{i:t_{i}\\leq t}\\left(1-{\\frac{d_{i}}{n_{i}}}\\right)}$. \n",
    " \n",
    "where $t_i$ is a time when at least one event happened, $d_i$ the number of event at time ${\\displaystyle t_{i}}$ and $n_i$ the individuals still in the study (neither dead nor censored) at time $t_i$.\n",
    "\n",
    "Intuitively, the product to compute ${\\hat{S}}(t)$ is on all time less than t, and thus allows to take into account all patients, even those *censored* before time $t$ because they got out of the reach of the study.\n",
    "\n",
    "\n",
    "Visually, a representation of a Kaplan-Meier estimator gives a decreasing staircase function, depicting an estimation of the survival function $S(t)$ ($S$ is supposed to be constant between two events)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.nonparametric import kaplan_meier_estimator \n",
    "\n",
    "x, y = kaplan_meier_estimator(E_train.astype(bool), y_train)\n",
    "plt.xlabel('t (time, in days)')\n",
    "plt.ylabel('S(t)')\n",
    "plt.title('Kaplan-Meier estimator')\n",
    "plt.step(x, y, where=\"post\")\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By allowing to estimate the survival function of a sample, the Kaplan-Meier estimator enables the comparison between two different samples (for instance, if you are studying a gene $g$, you can set a threshold of expression $k$, and compare the patients with a gene expression lower than $k$ to the patients with a gene expression higher than $k$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(X_train['MYC'].to_numpy())\n",
    "plt.title('Distribution of expression of gene MYC')\n",
    "plt.xlabel('Expression level')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the threshold to the mean, and create two subgroups :\n",
    "+ the patients ```high_MYC``` with a high-level of ```MYC```, e.g. > 19.5\n",
    "+ the patients ```low_MYC``` with a low-level of ```MYC```, e.g. < 19.5\n",
    "\n",
    "Obviously, one could optimize the threshold to improve the separation between the two subgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = X_train['MYC'].to_numpy().mean()\n",
    "print('Threshold is equal to %f' % threshold)\n",
    "\n",
    "low_MYC = X_train[X_train['MYC'] < threshold].index\n",
    "high_MYC = X_train[X_train['MYC'] > threshold].index\n",
    "\n",
    "x_low, y_low = kaplan_meier_estimator(E_train.astype(bool)[low_MYC], y_train[low_MYC])\n",
    "x_high, y_high = kaplan_meier_estimator(E_train.astype(bool)[high_MYC], y_train[high_MYC])\n",
    "plt.xlabel('t (time, in days)')\n",
    "plt.ylabel('S(t)')\n",
    "plt.title('Kaplan-Meier comparison between low-MYC and high-MYC')\n",
    "plt.step(x_low, y_low, where=\"post\", color='r', label='low MYC')\n",
    "plt.step(x_high, y_high, where=\"post\", color='g', label='high MYC')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title='Group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows that a high expression of the MYC gene is unfavourable to the patient's prognosis. This is consistent with the classification of this gene as an oncogene.\n",
    "\n",
    "But to compare the survival of different subgroups, other methods can be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The log-rank test <a name=\"log_rank_test\"></a>\n",
    "\n",
    "The log-rank test is a non-parametric hypothesis test allowing to compare the survival functions of two different populations. In practice, it's used to quantify the survival differences between two populations which can be observed by plotting their Kaplan-Meier estimation.\n",
    "\n",
    "Indeed, the log-rank test also allows to take into account censored lineage without loosing to much information.\n",
    "\n",
    "A few notations :\n",
    "\n",
    "For each time j, we write $N_{{1j}}$ and $N_{{2j}}$ the number of patients from each group in each group still studied at time j, and $N_{j}=N_{{1j}}+N_{{2j}}$. Let's write $O_{{1j}}$ and $O_{{2j}}$ the number of events (e.g deaths) in each group happening at time j, and similarly $O_{j}=O_{{1j}}+O_{{2j}}$.\n",
    "\n",
    "The idea of the log-rank test is to test the null hypothesis \"the two population have the same survival function\".\n",
    "\n",
    "Under this hypothesis, $O_{1j}$ follows an hypergeometric law with parameters $N_j$, $N_{1j}$ and $O_j$, and thus with expectation $E_{1j}$ and variance $V_j$.\n",
    "\n",
    "The convergence being garanteed by the Central Limit Theorem (Lyapunov version), we can define the random variable $${\\displaystyle Z={\\frac {\\sum _{j=1}^{J}(O_{1j}-E_{1j})}{\\sqrt {\\sum _{j=1}^{J}V_{j}}}}{\\xrightarrow {\\mathbb{L}}\\ N(0,1).}}$$\n",
    "\n",
    "It is then easy to quantify the ressemblance between the law of $Z$ and a standard normal law to see if the null hypothesis can be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.compare import compare_survival\n",
    "from problem import to_structured_array\n",
    "\n",
    "struct_E_y_train = to_structured_array(E_y_train)\n",
    "\n",
    "# Encode the group membership : 1 = high_MYC, 0 = low_MYC\n",
    "groups = np.ones(len(struct_E_y_train))\n",
    "groups[low_MYC] = 0.\n",
    "\n",
    "log_rank, p_value = compare_survival(struct_E_y_train, groups)\n",
    "print('The difference of survival between the two laws is %.2f' % log_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature design <a name=\"feature_design\"></a>\n",
    "\n",
    "As presented in the introduction, dimension reduction is at the heart of this challenge.\n",
    "\n",
    "We will therefore illustrate the two approaches (which are of course not exhaustive) that we mentioned in the introduction.\n",
    "\n",
    "Each of them is a research theme in itself: we do not pretend to explore them in a few lines of code. We merely sketch out possible directions for further research in order to design interesting features.\n",
    "\n",
    "Moreover, we don't make use of the aforementioned tools for survival analysis, that could of course be combined with the following features to improve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identification of the cancer driver genes <a name=\"identification_of_the_cancer_driver_genes\"></a>\n",
    "\n",
    "As presented in the introduction, a first idea would be to identify the *cancer driver genes*.\n",
    "\n",
    "A possible approach to do so would be to select only those genes that are most correlated, in absolute terms, with survival time. In order to do so, we first work only with the uncensored data.  \n",
    "\n",
    "We will apply Pearson correlation, but it relies on the assumption that the variable are a normally distributed, which is clearly not the case here, in particular for the ```time of survival```, as shown below. We therefore log-transform this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_uncensored = X_train[is_uncensored]\n",
    "y_uncensored = y_train[is_uncensored]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_time_to_death(y):\n",
    "    \"\"\"Compare the effect of the logarithm on the time.\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20,6))\n",
    "    sns.histplot(y, ax=ax1, stat='probability',bins=25)\n",
    "    ax1.set_title('Distribution of the time to death')\n",
    "    ax1.set_xlabel('Time to death')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "\n",
    "    log_time = np.log(y)\n",
    "    sns.histplot(log_time, ax=ax2, stat='probability', color='r', bins=25)\n",
    "    ax2.set_title('Distribution of the logarithm of the time to death')\n",
    "    ax2.set_xlabel('Time to death (log)')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    plt.show()\n",
    "    \n",
    "display_time_to_death(y_uncensored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logarithmic transformation slightly improves the time distribution and makes it a little more normal.\n",
    "Let's compute the correlation of the expression of each gene with the time to death."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlations(X, y_log):\n",
    "    \"\"\"Compute the correlation and the associated p-values between the genes and the survival times.\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    scaled_X = scaler.fit_transform(X)\n",
    "    gene_index, corr_r, p_values = [], [], []\n",
    "    corr_df = pd.DataFrame(columns = {'genes', 'correlation', 'p_values'})\n",
    "    for i, gene in enumerate(scaled_X.T):\n",
    "        r, p_val = pearsonr(gene, log_time)\n",
    "        if not math.isnan(r): # If the correlation is valid\n",
    "            gene_index.append(X.columns[i])\n",
    "            corr_r.append(r)\n",
    "            p_values.append(p_val)\n",
    "    \n",
    "    corr_df = pd.DataFrame({'correlation' : corr_r, \n",
    "            'p_value' : p_values} , index = gene_index) \n",
    "    \n",
    "    return corr_df\n",
    "\n",
    "\n",
    "def display_correlations(corr_df):\n",
    "    \"\"\"Display the correlation and the p-values.\"\"\"\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(20,10))\n",
    "    \n",
    "    sns.histplot(corr_df['correlation'].to_numpy(), ax=ax1, stat='probability')\n",
    "    ax1.set_title('Distribution of the correlation between the genes expression and the time before death')\n",
    "    ax1.set_xlabel('Correlation value')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    \n",
    "    sns.histplot(corr_df['p_value'].to_numpy(), ax=ax2, stat='probability')\n",
    "    ax2.set_title('Distribution of the p-values associated to correlations.')\n",
    "    ax2.set_xlabel('p-values')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    \n",
    "    probplot(corr_df['correlation'].to_numpy().flatten(), dist=\"norm\",plot=ax3)\n",
    "    ax3.set_title('QQ-plot between the normal distribution and the correlation distribution.')\n",
    "    \n",
    "    probplot(corr_df['p_value'].to_numpy().flatten(), dist=\"uniform\", plot=ax4)\n",
    "    ax4.set_title('QQ-plot between the uniform distribution and the p-values distribution.')\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_time = np.log(y_uncensored)\n",
    "correlation_df = compute_correlations(X_uncensored, log_time)\n",
    "display_correlations(correlation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, the distribution of correlations seems close to the normal law... The comparison between the two laws can be refined by means of a Q-Q plot. It allows to compare graphically an empirical distribution with its theorical counterpart, by comparing their quantiles.\n",
    "\n",
    "We expect the correlation values to be normally distributed if it is only a matter of chance. The associated p-values would then be uniformly distributed between 0 and 1. We thus compare the correlations (resp. p-values) distribution with the normal (resp. the uniform) distribution.\n",
    "\n",
    "We thus observe an excess of genes correlated (in absolute value) with survival time compared to a distribution purely due to chance, which is encouraging : these highly correlated genes are good candidate to be cancer driver genes. (The positively correlated would thus be oncogenes and the negatively correlated would thus be tumour suppressor genes.)\n",
    "\n",
    "Unfortunately, the distribution of p-values, which corresponds to the significance of the observed correlations, coincides rather well with a random distribution, which prevents us from selecting genes based on a reliable statistical way.\n",
    "\n",
    "We can nevertheless select a first pool of genes (the top-correlated genes in absolute value), and use them as our first feature set : ```feature_1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_threshold = 0.25\n",
    "high_corr = correlation_df[np.abs(correlation_df['correlation']) > correlation_threshold]\n",
    "high_correlation_genes = high_corr.index.to_numpy()\n",
    "feature_1 = X_uncensored.filter(high_correlation_genes)\n",
    "print('We keep %i genes.' % feature_1.shape[1])\n",
    "feature_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to keep the genes based on a statistical criterion, we need to include the information of censored patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considering the whole dataset <a name=\"considering_the_whole_dataset\"></a>\n",
    "\n",
    "As their censorship is independent of the survival time (by hypothesis), we can, as before, correlate their transcriptome with their survival time (although it is truncated for the censored patient). This should similarly allow us to isolate the cancer driver genes, and hopefully, with more data, to observe a deviation from the p-values distribution.\n",
    "\n",
    "We thus proceed as precedently :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_time_to_death(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_time = np.log(y_train)\n",
    "correlation_df = compute_correlations(X_train, log_time)\n",
    "display_correlations(correlation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-values are not randomly distributed at all, and we observe in particular an enrichment in small p-values, which therefore correspond to significant correlations.\n",
    "\n",
    "Let's extract the associated genes to create a second pool of features. We choose to keep the genes with a p-value < 0.05, i.e. with a 95% confidence interval.\n",
    "\n",
    "But this lead to another question : how do we take into account the fact that we have tested a lot of different hypothesis ? Indeed, even if no gene has a effect on breast cancer mortality, we would still expect 5% of the 35000 genes to come out as significant with a confidence level of 0.05. This makes eventually 1750 false positive ! \n",
    "\n",
    "On the other hand, we know that some genes must have an effect, as the p-values we found by testing the genes one by one do not exhibit complete randomness, as shown in the above QQ-plot.\n",
    "\n",
    "To overcome the issue of false positives, several multiple hypothesis correction exist, controlling either the *family-wise error rate*, that is the probability of keeping a gene which has no effect on the mortality, or the *false discovery rate*, controlling only the expected number of false discoveries.\n",
    "\n",
    "The goal here being to identify relevant genes, in order to keep only relevant features, we have chosen to use the more conservative controls (the *family-wise error rate* controls). \n",
    "We thus choose a confidence level $\\alpha = 0.05$ for the family-wise error rate, and we extract a group of genes such as the probability of making 0 type 1 error (saying there is an effect on mortality when there is none) is > 0.95.\n",
    "\n",
    "We do this using the [*Bonferroni correction*](https://en.wikipedia.org/wiki/Bonferroni_correction). it consists of a strengthening of the restriction on the p-value:\n",
    "if we want an $\\alpha$-confidence level, and we test $m$ hypotheses (here $m$ = number of genes), then we only keep those with a p-value lower than $\\frac{\\alpha}{m}$, and create a second feature (```feature_2```) with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonferroni_alpha = 0.05 / X_uncensored.shape[1]\n",
    "print('The value of the Bonferroni correction is %f' % bonferroni_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_corr = correlation_df[np.abs(correlation_df['p_value']) < bonferroni_alpha]\n",
    "sign_correlation_genes = sign_corr.index.to_numpy()\n",
    "feature_2 = X_train.filter(sign_correlation_genes)\n",
    "print('We keep %i genes.' % feature_2.shape[1])\n",
    "feature_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatives to naïve Pearson's correlation are possible. In particular, the same procedure can be used with an univariate Cox Regression (see the baseline model), which can return the p-value associated to each feature (here, to each gene).\n",
    "Selecting those with the lowest p-value has shown encouraging results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing with the Cancer Gene Census database <a name=\"comparing_with_the_cancer_gene_census_database\"></a>\n",
    "\n",
    "Statistical studies on the genes most frequently mutated in patients' tumours have made it possible to identify genes frequently mutated in cancer, and thus to create cancer driver genes databases.\n",
    "\n",
    "Several lists exist: we provide one, extracted from the [CENSUS](https://cancer.sanger.ac.uk/census) database, one of the most widely used cancer driver genes databases. (We simplified the original database to retain only the list of the consensus genes, as well as a measure of our confidence in each of these genes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem import get_census\n",
    "census = get_census()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_consensus = set(census['Gene Symbol'].to_numpy())\n",
    "print('The CENSUS database contains %i genes.' % len(genes_consensus))\n",
    "common_genes = genes_consensus.intersection(set(X_train.columns))\n",
    "print('%i are both part of the CENSUS database and of X_train.' % len(common_genes))\n",
    "\n",
    "census.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have information on the whole genome for X_train, but \"only\" for the major part of it. That is why some genes from the CENSUS database are missing.\n",
    "\n",
    "The column ```Gene Symbol``` provides the list of the genes, and the ```Tier``` is a measure of the confidence (the lower the better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tier can takes %i values :' % len(np.unique(census['Tier'].to_numpy())), np.unique(census['Tier'].to_numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the retained genes in ```feature_1``` and ```feature_2``` to the known cancer driver genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genes_feature_1 = set(feature_1.columns)\n",
    "genes_feature_2 = set(feature_2.columns)\n",
    "\n",
    "print('They are %i genes both in the Census database and in feature_1.' % len(genes_consensus.intersection(genes_feature_1)))\n",
    "print('They are %i genes both in the Census database and in feature_2.' % len(genes_consensus.intersection(genes_feature_2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This comparison shows that the second approach, statistically reliable, seems to have slightly better results. However, both approachs seem to retain only very few known oncogenes. It is because it is very hard for a gene to be recorded in the Census database, and this database is thus highly incomplete. Moreover, the CENSUS database is based on mutated genes, while we are looking at gene expression.\n",
    "\n",
    "This poor correspondance between our features and the known oncogenes is thus not alarming, and we will define a third feature, ```feature_3``` containing the genes from the CENSUS database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_3 = X_train.filter(common_genes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can once more display the correlation and the associated p-values for each gene of ```feature_3```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_time = np.log(y_train)\n",
    "correlation_df = compute_correlations(feature_3, log_time)\n",
    "display_correlations(correlation_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Leveraging the gene network topology <a name=\"leveraging_the_gene_network_topology\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to represent the genes topology is to compute a graph based on the correlation between each gene. However, we cannot include all genes (the computation of the graph would take way to much time), so we need to define a treshold for the correlation and keep only the genes with a correlation superior to this treshold.\n",
    "In order to do that, we first compute the correlation between 1000 random genes and plot the histogram, we then visually assign the treshold (the treshold should depend on your computation power and your time available). We can see that the correlation matrix is then really sparse and will be more easy to manipulate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_genes = 1000\n",
    "random_genes = np.random.choice(X_train.columns, size=n_genes, replace=False)\n",
    "corr = X_train[random_genes].corr().abs()\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(21,5))\n",
    "ax1.matshow(corr)\n",
    "ax1.set_title(\"Correlation matrix between 1000 random genes\")\n",
    "\n",
    "unstacked_corr = corr.unstack()\n",
    "ax2.hist(unstacked_corr)\n",
    "ax2.set_title(\"Histogram of the correlation\")\n",
    "ax3.hist(unstacked_corr, range=(0.5,0.999))\n",
    "ax3.set_title(\"Histogram of the correlation (between 0.5 and 1)\")\n",
    "plt.show()\n",
    "\n",
    "treshold = .7\n",
    "corr_filtered = corr[corr>treshold]\n",
    "sns.heatmap(corr_filtered, cmap=\"Greens\")\n",
    "plt.title(\"Matrix of the tresholded correlation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we defined our treshold, we need to compute all the genes correlations, this step can take lot of time so we decided to restrain the number of genes observed, we selected all the genes from the census database and 1500 other random genes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_genes = 1500\n",
    "recalculate_corrs = False\n",
    "census_expression = census['Gene Symbol']\n",
    "all_cols = set(X_train.columns)\n",
    "set_census = all_cols.intersection(census_expression)\n",
    "\n",
    "if recalculate_corrs:\n",
    "    all_corr = pd.DataFrame()\n",
    "    visited_cols = set()\n",
    "\n",
    "    cols_census = list(set_census)\n",
    "    cols_not_census = list(all_cols - set_census)\n",
    "    random.shuffle(cols_not_census)\n",
    "\n",
    "    cols =  cols_census + cols_not_census[0:n_genes]\n",
    "    for col in tqdm(cols):\n",
    "        visited_cols.add(col)\n",
    "        corr = X_train[all_cols-visited_cols].corrwith(X_train[col]).abs()\n",
    "        filtered_corr = corr[corr>0.1]\n",
    "        filtered_corr_df = pd.DataFrame({'gene1': col,'gene2':filtered_corr.index, 'corr':filtered_corr.values})\n",
    "        all_corr = all_corr.append(filtered_corr_df)\n",
    "    all_corr.to_csv('data/corrs.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to represent this correlation as a graph. For this we use the library networkx wich is really powerful and simple to use. We first draw the graph and the subgraph of genes in the census database (red) and genes outside of the database (blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "corrs = pd.read_csv('data/corrs.csv')\n",
    "G = nx.from_pandas_edgelist(corrs, 'gene1', 'gene2', 'corr')\n",
    "\n",
    "colors = []\n",
    "census_graph = []\n",
    "not_census_graph = []\n",
    "for node in G.nodes:\n",
    "    if node in set_census:\n",
    "        colors.append('red')\n",
    "        census_graph.append(node)\n",
    "    else:\n",
    "        colors.append('blue')\n",
    "        not_census_graph.append(node)\n",
    "\n",
    "nx.draw(G, node_size=1, node_color=colors, width=0.1)\n",
    "plt.show()\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(30,7))\n",
    "nx.draw(G.subgraph(census_graph), node_size=1, width=0.1, ax=ax1, node_color= 'red')\n",
    "nx.draw(G.subgraph(not_census_graph), node_size=1,width=0.1, ax=ax2, node_color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try to find the communities in this graph, we will then try to find the communities which when it is expressed make the survivability vary the most, this community will be our selected fetures. In order to find the communities, we will use the Clauset-Newman-Moore greedy modularity maximization algorithm (implemented as the function greedy_modularity_communities) packaged with the networkx algorithm, some more powerful solution are possible but it is beyond the goal of this challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "\n",
    "giant = max([G.subgraph(c) for c in nx.connected_components(G)], key=len)\n",
    "communities = list(greedy_modularity_communities(G))\n",
    "print('Number of communities detected : ', len(communities))\n",
    "df_train =  pd.DataFrame({ 'y' : y_train })\n",
    "corr = np.array([])\n",
    "for c, v_c in enumerate(communities):\n",
    "    corr = np.append(corr, X_train[v_c].corrwith(df_train['y']).abs().mean())\n",
    "best_communities = np.argsort(corr)[-22:]\n",
    "features_graph = []\n",
    "for c in best_communities:\n",
    "    features_graph += list(communities[c])\n",
    "features_graph = set(features_graph)\n",
    "print(\"Number of features : \", len(features_graph))\n",
    "feature_4 = X_train.filter(features_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric <a name=\"metric\"></a>\n",
    "\n",
    "An adaptation of the AUROC (Area Under the Receiver Operating Characteristic) score, more commonly known as AUC, is used.\n",
    "This measure usually counts the number of correctly ordered pairs, i.e. for two patients $i$ and $j$, compares which of the two died first in reality (in ``y_true``) and looks whether this ranking is correctly predicted by the model (in ``y_pred``).\n",
    "\n",
    "However, the censored data on the right forces us to adapt this score. One way to do this is to compensate the score obtained by the inverse of the censoring probability $G$ (itself estimated using a Kaplan-Meier model with the data in ``E_train``).\n",
    "\n",
    "The calculation is carried out using the following formula :\n",
    "\n",
    "$$\n",
    "C_{index} = \\frac{\\sum_{i=1}^{n} \\sum_{j=1}^{n} E_{i} \\{ G(y^{true}_{i}) \\}^{-2} I(y_{i} < y_{j}, y_{i} < \\tau) I(y^{pred}_{i} < y^{pred}_{j})}{\\sum_{i=1}^{n} \\sum_{j=1}^{n} E_{i} \\{ G(y_{i}) \\}^{-2} I(y_{i} < y_{j}, y_{i} < \\tau)}\n",
    "$$\n",
    "\n",
    "where each patient $i$ is described by the tuple $(X_{i}, E_{i}, y_{i})$. $G(.)$ designs the censoring distribution, e.g. $G(t) = P(D > t)$, where $D$ is the time of censoring. $I$ is the indicator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem import ConcordanceIndex\n",
    "ci = ConcordanceIndex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model <a name=\"baseline_model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Cox regression<a name=\"cox_regression\"></a>\n",
    "\n",
    "The Cox regression is the equivalent of the logistic regression for survival analysis.\n",
    "Let's define the *hazard rate*\n",
    "$$\\lambda(t)=\\lim _{d t \\rightarrow 0} \\frac{\\operatorname{Pr}(t \\leq T<t+d t)}{d t \\cdot S(t)}=-\\frac{S^{\\prime}(t)}{S(t)}$$\n",
    "defined as the probability of dying at a certain time t given the fact that the patient survived until time t.\n",
    "The Cox regression belong to the class of *proportional hazard models*, that is models which make the assumption that each unit increase for a covariate multiply the hazard rate of the patient by a fixed amount. For instance, taking a certain drug could halve the probability of dying from cancer at each time t.\n",
    "\n",
    "This idea is formalized this way : \n",
    "\n",
    "Let $X_{i}=\\left\\{X_{i 1}, \\ldots X_{i p}\\right\\}$ be the covariate values for a certain patient i.\n",
    "Then we model the hazard rate $\\lambda$ by \n",
    "<p style=\"color:#FF0000\";>$${\\lambda\\left(t | X_{i}\\right)=\\lambda_{0}(t) \\exp \\left(\\beta_{1} X_{i 1}+\\cdots+\\beta_{p} X_{i p}\\right)=\\lambda_{0}(t) \\exp \\left(X_{i} \\cdot \\beta\\right)}$$</p>.\n",
    "\n",
    "Let's  note that only the base hazard rate is time-dependant : the effect of each covariate on the *hazard rate* is assumed to be the same across time.\n",
    "\n",
    "This property is the main \"trick\" of Cox regression : it allows to get rid of the hard-to-estimate hazard rate by taking a ratio. To be more precise, the likelihood of an event to be observed on patient i at time $Y_i$ is $$L_{i}(\\beta)=\\frac{\\lambda\\left(Y_{i} | X_{i}\\right)}{\\sum_{j : Y_{j} \\geq Y_{i}} \\lambda\\left(Y_{i} | X_{j}\\right)}=\\frac{\\lambda_{0}\\left(Y_{i}\\right) \\theta_{i}}{\\sum_{j : Y_{j} \\geq Y_{i}} \\lambda_{0}\\left(Y_{i}\\right) \\theta_{j}}=\\frac{\\theta_{i}}{\\sum_{j : Y_{j} \\geq Y_{i}} \\theta_{j}},$$ with $\\theta_{j}=\\exp \\left(X_{j} \\cdot\\beta\\right)$ and the summation being on all the patient still alive and in the study at time $Y_i$.\n",
    "\n",
    "Skipping the most technical details, the idea is then to model all patients as independant and to maximize the product likelihood.\n",
    "\n",
    "Here, we use the implementation from [```scikit-survival```](https://scikit-survival.readthedocs.io/en/latest/api/generated/sksurv.linear_model.CoxPHSurvivalAnalysis.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "\n",
    "def get_unexpressed_columns(X_train):\n",
    "    mean_x = X_train.mean()\n",
    "    unexpressed_columns = X_train.columns[(mean_x == 0.).to_numpy()]\n",
    "    unexpressed_columns = [column.split('\\n')[0] for column in unexpressed_columns]\n",
    "    return unexpressed_columns\n",
    "    \n",
    "    \n",
    "class CoxRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self):\n",
    "        pca = PCA(n_components=10)\n",
    "        regressor = CoxPHSurvivalAnalysis()\n",
    "        self.regr = Pipeline([('pca', pca), ('regressor', regressor)])\n",
    "        return \n",
    "\n",
    "    def fit(self, X, E_y=None):\n",
    "        self.to_drop_columns = get_unexpressed_columns(X)\n",
    "        X.drop(columns=self.to_drop_columns, inplace=True)\n",
    "        struct_E_y = to_structured_array(E_y)\n",
    "        self.regr.fit(X, struct_E_y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X.drop(columns=self.to_drop_columns, inplace=True)\n",
    "        risk_pred = self.regr.predict(X)\n",
    "        y_pred = (max(risk_pred) + 1) - risk_pred\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "class LinearRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self):\n",
    "        pca = PCA(n_components=10)\n",
    "        regressor = LinearRegression(n_jobs=-1)\n",
    "        self.regr = Pipeline([('pca', pca), ('regressor', regressor)])\n",
    "        return \n",
    "\n",
    "    def fit(self, X, E_y=None):\n",
    "        self.to_drop_columns = get_unexpressed_columns(X)\n",
    "        X.drop(columns=self.to_drop_columns, inplace=True)\n",
    "        \n",
    "        y_to_predict = E_y[:,1] # We are only interested in predicting the survival time, not the censoring\n",
    "        \n",
    "        self.regr.fit(X, y_to_predict)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X.drop(columns=self.to_drop_columns, inplace=True)\n",
    "        return self.regr.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the model and compare it to a classic linear regressor. We use for this a PCA to reduce the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, E_y_train = get_train_data()\n",
    "X_test, E_y_test = get_test_data()\n",
    "\n",
    "cox_regressor = CoxRegressor()\n",
    "cox_regressor.fit(X_train, E_y_train)\n",
    "y_pred = cox_regressor.predict(X_test)\n",
    "cox_score = ci(E_y_test, y_pred)\n",
    "\n",
    "linear_regressor = LinearRegressor()\n",
    "linear_regressor.fit(X_train, E_y_train)\n",
    "y_pred = linear_regressor.predict(X_test)\n",
    "linear_score = ci(E_y_test, y_pred)\n",
    "\n",
    "print('Performance of the linear model : %.3f\\nPerformance of the Cox model : %.3f' %(linear_score, cox_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the linear model performs worse than a random regressor, while the Cox Regression performs significantly better. We will thus use it to compare the relevance of the different sets of features we designed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "def get_feature(num_feature):\n",
    "    genes_to_drop = []\n",
    "     # select the genes to drop and record them in genes_to_drop ( a list with the column names)\n",
    "    \n",
    "    ### TODO\n",
    "    if num_feature == 1:\n",
    "        pass\n",
    "    elif num_feature == 2:\n",
    "        pass\n",
    "    elif num_feature == 3:\n",
    "        pass\n",
    "    elif num_feature == 4:\n",
    "        pass\n",
    "    else:\n",
    "        print('Only 4 different feature sets.')\n",
    "    ### END TODO\n",
    "        \n",
    "    # Drop these columns\n",
    "    feature_transformer = make_column_transformer(('drop', genes_to_drop), remainder='passthrough')\n",
    "    return feature_transformer\n",
    "\n",
    "\n",
    "class CoxRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, num_feature):\n",
    "        feature_transformer = get_feature(num_feature)\n",
    "        regressor = CoxPHSurvivalAnalysis()\n",
    "        self.regr = Pipeline([('feature_transformer', feature_transformer), ('regressor', regressor)])\n",
    "        return \n",
    "\n",
    "    def fit(self, X, E_y=None):\n",
    "        struct_E_y = to_structured_array(E_y)\n",
    "        self.regr.fit(X, struct_E_y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        risk_pred = self.regr.predict(X)\n",
    "        y_pred = (max(risk_pred) + 1) - risk_pred\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission <a name=\"submission\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should implement a class ```Regressor``` in the file ```regressor.py```, provided in the ```starting_kit``` repository. This regressor should implement a ```predict``` method, which returns the survival time for each patient.\n",
    "\n",
    "You can then test your submission by running the following command :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ramp-test --submission starting_kit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit your code, you can refer to the <a href=\"https://paris-saclay-cds.github.io/ramp-docs/ramp-workflow/stable/using_kits.html\">online documentation</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}